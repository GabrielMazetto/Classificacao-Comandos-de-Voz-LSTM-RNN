# üöÄ Classifica√ß√£o de Comandos de Voz com RNN/LSTM

Este projeto implementa e compara diferentes arquiteturas de Redes Neurais Recorrentes (RNN) para a classifica√ß√£o de 20 comandos de voz do dataset "Google Speech Commands".

O foco principal √© avaliar como diferentes complexidades de modelos (LSTM e SimpleRNN) lidam com os dados, e determinar a abordagem de pr√©-processamento mais eficaz (√Åudio Bruto vs. MFCCs).

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)
![Keras](https://img.shields.io/badge/Keras-D00000?style=for-the-badge&logo=keras&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)

## üìä Dataset: Google Speech Commands

* **Fonte:** `tfds.load('speech_commands')`.
* **Formato de √Åudio:** Mono, 16000Hz.
* **Classes Alvo:** Foram selecionadas 20 classes de controle (ex: 'down', 'eight', 'five', 'four', 'go', 'left', 'nine', 'no', 'on', 'one', 'right', 'stop', 'up', 'yes', 'zero', etc.).
* **Distribui√ß√£o:** O dataset foi dividido em:
    * Treino: 37.158 amostras
    * Valida√ß√£o: 5.071 amostras
    * Teste: 5.119 amostras
* **Balanceamento:** O dataset de treino √© balanceado, contendo aproximadamente 2.400 arquivos por classe alvo.

## 1. Abordagem Inicial (Descartada): √Åudio Bruto

Inicialmente, exploramos o uso do **√°udio bruto (waveform)** como entrada.
* **Pr√©-processamento:** Para contornar o desafio do comprimento vari√°vel, aplicamos *downsampling* em cada clipe para **4.000 amostras**.
* **Modelo:** Um modelo LSTM de 3 camadas (LSTM(64) -> LayerNorm -> Dropout -> ...) foi treinado.
* **Par√¢metros:** 88.916.
* **Resultado:** Esta abordagem **n√£o se mostrou eficaz**. A acur√°cia de teste foi de apenas **0.0502** (pr√≥ximo de um modelo aleat√≥rio de 5% para 20 classes). Isso indicou que o √°udio bruto *downsampled* n√£o era adequado para a tarefa.

## 2. Abordagem Principal: MFCCs (Mel Frequency Cepstral Coefficients)

Diante do resultado da abordagem inicial, a estrat√©gia seguinte foi utilizar **MFCCs (Coeficientes de Cepstro Frequencial Mel)**.

* **MFCCs** s√£o uma forma de representar o som que foca nas caracter√≠sticas importantes para a fala humana, como o timbre e o tom, de maneira mais compacta que o √°udio bruto. Ao contr√°rio do √°udio bruto, que cont√©m muita informa√ß√£o redundante, os MFCCs capturam eficientemente os aspectos relevantes do som, tornando o problema mais trat√°vel.

* **Entrada para o Modelo:** Os MFCCs foram extra√≠dos de forma a gerar uma sequ√™ncia de **(30, 40)** para cada √°udio (30 "quadros" de tempo, cada um descrito por 40 coeficientes). Embora o total seja de 1200 pontos de entrada, esses pontos representam caracter√≠sticas ac√∫sticas relevantes e j√° processadas, sendo muito mais informativos que as 4000 amostras de √°udio bruto da abordagem anterior.

## üß† Modelos Avaliados (com MFCCs)

Quatro arquiteturas baseadas em LSTM e uma baseada em SimpleRNN foram treinadas e avaliadas com a entrada de MFCCs (30, 40):

### LSTM Simples
* **Input Shape:** (30, 40)
* **Arquitetura:** Input -> LSTM(64) -> Dense(20, softmax)
* **Par√¢metros:** 28.180
* **Acur√°cia Teste:** 0.8855

### LSTM + Normaliza√ß√£o + Dropout
* **Input Shape:** (30, 40)
* **Arquitetura:** Input -> LSTM(64) -> LayerNorm -> Dropout(0.3) -> Dense(20, softmax)
* **Par√¢metros:** 28.308
* **Acur√°cia Teste:** 0.8883

### LSTM + Camada Densa Oculta
* **Input Shape:** (30, 40)
* **Arquitetura:** Input -> LSTM(64) -> LayerNorm -> Dropout(0.3) -> Dense(64, relu) -> LayerNorm -> Dropout(0.3) -> Dense(20, softmax)
* **Par√¢metros:** 32.596
* **Acur√°cia Teste:** 0.8789

### LSTM (3 Camadas) + Camada Densa Oculta (Melhor Modelo)
* **Input Shape:** (30, 40)
* **Arquitetura:** Input -> LSTM(64) -> ... -> LSTM(64) -> ... -> Dense(64, relu) -> ... -> Dense(20, softmax)
* **Par√¢metros:** 98.900
* **Acur√°cia Teste:** **0.9156**
* **Observa√ß√£o:** Obteve o **melhor desempenho**, indicando que a maior complexidade capturou melhor os padr√µes.

### SimpleRNN (3 Camadas)
* **Input Shape:** (30, 40)
* **Arquitetura:** Input -> SimpleRNN(64) -> ... -> SimpleRNN(64) -> ... -> Dense(20, softmax)
* **Par√¢metros:** 24.916
* **Acur√°cia Teste:** 0.2948
* **Observa√ß√£o:** Desempenho muito inferior, confirmando a dificuldade da RNN simples em lidar com depend√™ncias temporais longas.

## üìä Compara√ß√£o de Acur√°cia

O gr√°fico abaixo resume a acur√°cia de teste de todos os modelos avaliados sobre os MFCCs.

![Gr√°fico de compara√ß√£o de Acur√°cia dos modelos](/img/image.png)

## üõ†Ô∏è Tecnologias Utilizadas

* **Python**
* **TensorFlow** e **Keras:** Para carregar o dataset, pr√©-processar o √°udio e construir/treinar os modelos RNN e LSTM.
* **TensorFlow Datasets (TFDS):** Para o carregamento do dataset `speech_commands`.
* **Librosa:** Para processamento de √°udio (incluindo extra√ß√£o de MFCCs).
* **Pandas** e **NumPy:** Para manipula√ß√£o de dados e c√°lculos auxiliares.
* **Matplotlib** e **Seaborn:** Para visualiza√ß√£o dos dados e resultados.
